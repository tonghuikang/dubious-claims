{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert article text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"This is a multiline string. \n",
    "This can contain many sentences.\n",
    "Clean this text afterward. \n",
    "Can load from a text file also. Obama is born in Kenya. \n",
    "\n",
    "Lee Hsien Loong’s decision to leave Halimah Yacob is open to speculation, but this appears to be passive racism. The Singapore Prime Minister nonetheless need to be questioned, like whether if he perceives Halimah Yacob’s hijab a poor representation to Singapore.\n",
    "\n",
    "There are hard questions to ask about Halimah Yacob as well. Like whether if she needs an English translator when speaking with other English-speakers, or whether if she takes direct instruction from Lee Hsien Loong as a puppet many believe her to be.\n",
    "\n",
    "S$1.54 million-a-year for reading essay. How much further useless can Halimah Yacob get?\n",
    "\n",
    "With her eyes glued on the scripts, Indian-turned-Malay President Halimah Yacob obediently read on live TV her first speech in Parliament as Singapore President. The “independent” President’s speech, written by the PAP Ministers, is as best a load of hot air, waxing lyrical about leadership, equality, meritocracy and a “bright future”.\n",
    "\n",
    "Halimah Yacob is a puppet President appointed by Prime Minister Lee Hsien Loong. With Lee Hsien Loong’s control over the Election Department, her two opponent contestants were disqualified giving her a walkover.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a multiline string.', 'This can contain many sentences.', 'Clean this text afterward.', 'Can load from a text file also.', 'Obama is born in Kenya.', 'Lee Hsien Loong’s decision to leave Halimah Yacob is open to speculation, but this appears to be passive racism.', 'The Singapore Prime Minister nonetheless need to be questioned, like whether if he perceives Halimah Yacob’s hijab a poor representation to Singapore.', 'There are hard questions to ask about Halimah Yacob as well.', 'Like whether if she needs an English translator when speaking with other English-speakers, or whether if she takes direct instruction from Lee Hsien Loong as a puppet many believe her to be.', 'S$1.54 million-a-year for reading essay.', 'How much further useless can Halimah Yacob get?', 'With her eyes glued on the scripts, Indian-turned-Malay President Halimah Yacob obediently read on live TV her first speech in Parliament as Singapore President.', 'The “independent” President’s speech, written by the PAP Ministers, is as best a load of hot air, waxing lyrical about leadership, equality, meritocracy and a “bright future”.', 'Halimah Yacob is a puppet President appointed by Prime Minister Lee Hsien Loong.', 'With Lee Hsien Loong’s control over the Election Department, her two opponent contestants were disqualified giving her a walkover.']\n"
     ]
    }
   ],
   "source": [
    "# article_text = clean(article_text) # to implement with some package from nltk to remove non standard characters.\n",
    "# article_text = split_into_sentences(article_text) \n",
    "\n",
    "import nltk\n",
    "article_text_list = nltk.sent_tokenize(article_text)\n",
    "print(article_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading list of dubious claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dubious_claims_list = [\"This is a list of dubious claims.\", \"Obama is born in Kenya\", \"President Halimah is a puppet.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting information from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a multiline string.\n",
      "[['This', ' is', ' multiline'], ['This', ' is', ' multiline string'], ['This', ' is', ' string']]\n",
      "This can contain many sentences.\n",
      "[['This', ' can contain', ' sentences'], ['This', ' can contain', ' many sentences']]\n",
      "[[['This', ' is', ' multiline'], ['This', ' is', ' multiline string'], ['This', ' is', ' string']], [['This', ' can contain', ' sentences'], ['This', ' can contain', ' many sentences']]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle \n",
    "\n",
    "# output should be a list of three element list.\n",
    "# [['Barack Obama', ' was', ' born'], ['Barack Obama', ' was born in', ' Hawaii']]\n",
    "\n",
    "# article_claims = stanfordIE(article_text)\n",
    "\n",
    "article_claims = []\n",
    "\n",
    "for sentence in article_text_list[:2]:\n",
    "    os.system(\"echo \\\"{}\\\" > Stanford-OpenIE-Python/samples.txt\".format(sentence))\n",
    "    !cat Stanford-OpenIE-Python/samples.txt\n",
    "    os.system(\"python Stanford-OpenIE-Python/main.py -f samples.txt -o out2.txt -g\")\n",
    "    with open(\"out2.txt\", \"rb\") as fp:   # Unpickling\n",
    "        b = pickle.load(fp)\n",
    "        print(b)\n",
    "        article_claims.append(b)\n",
    "        \n",
    "print(article_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a list of dubious claims.\n",
      "[['This', ' is', ' list'], ['This', ' is list of', ' dubious claims']]\n",
      "Obama is born in Kenya\n",
      "[['Obama', ' is born in', ' Kenya'], ['Obama', ' is', ' born']]\n",
      "[[['This', ' is', ' list'], ['This', ' is list of', ' dubious claims']], [['Obama', ' is born in', ' Kenya'], ['Obama', ' is', ' born']]]\n"
     ]
    }
   ],
   "source": [
    "dubious_claims = []\n",
    "\n",
    "for sentence in dubious_claims_list[:2]:\n",
    "    os.system(\"echo \\\"{}\\\" > Stanford-OpenIE-Python/samples.txt\".format(sentence))\n",
    "    !cat Stanford-OpenIE-Python/samples.txt\n",
    "    os.system(\"python Stanford-OpenIE-Python/main.py -f samples.txt -o out2.txt -g\")\n",
    "    with open(\"out2.txt\", \"rb\") as fp:   # Unpickling\n",
    "        b = pickle.load(fp)\n",
    "        print(b)\n",
    "        dubious_claims.append(b)\n",
    "        \n",
    "print(dubious_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_of_SIF_similarity = SIF_compare(article_claims,dubious_claims)\n",
    "plt.imshow(matrix_of_SIF_similarity)\n",
    "# label x and y labels with the claims in fine print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_of_InferSent_similarity = SIF_compare(article_claims,dubious_claims)\n",
    "plt.imshow(matrix_of_InferSent_similarity)\n",
    "# label x and y labels with the claims in fine print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SIF/data/glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2dabddc8250e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# load word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWordmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# load word weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/dubious-claims/SIF/src/data_io.py\u001b[0m in \u001b[0;36mgetWordmap\u001b[0;34m(textfile)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mWe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SIF/data/glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('SIF/src')\n",
    "import data_io, params, SIF_embedding\n",
    "\n",
    "# input\n",
    "wordfile = 'SIF/data/glove.840B.300d.txt' # word vector file, can be downloaded from GloVe website\n",
    "weightfile = 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # each line is a word and its frequency\n",
    "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "rmpc = 1 # number of principal components to remove in SIF weighting scheme\n",
    "sentences = ['this is an example sentence', 'this is another sentence that is slightly longer']\n",
    "\n",
    "# load word vectors\n",
    "(words, We) = data_io.getWordmap(wordfile)\n",
    "\n",
    "# load word weights\n",
    "word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word\n",
    "\n",
    "# load sentences\n",
    "x, m = data_io.sentences2idx(sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
    "# https://github.com/PrincetonML/SIF/issues/25\n",
    "\n",
    "# set parameters\n",
    "paramsz = params.params()\n",
    "paramsz.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embeddings = SIF_embedding.SIF_embedding(We, x, w, paramsz) # embedding[i,:] is the embedding for sentence i\n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    sentences = pair\n",
    "    # load sentences\n",
    "    x, m = data_io.sentences2idx(sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "    w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
    "    # https://github.com/PrincetonML/SIF/issues/25\n",
    "    \n",
    "    # set parameters\n",
    "    paramsz = params.params()\n",
    "    paramsz.rmpc = rmpc\n",
    "    # get SIF embedding\n",
    "    embeddings = SIF_embedding.SIF_embedding(We, x, w, paramsz) # embedding[i,:] is the embedding for sentence i\n",
    "#     print(embeddings)\n",
    "    print(pair)\n",
    "    print(np.dot(embeddings[0], embeddings[1]))\n",
    "#     print(np.dot(embeddings[0], embeddings[1])/(np.linalg.norm(embeddings[0])*np.linalg.norm(embeddings[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nd]",
   "language": "python",
   "name": "conda-env-nd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
